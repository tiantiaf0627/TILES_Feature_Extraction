{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning on emotion clustering label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Other libraries\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.curdir, '../', 'util'))\n",
    "from load_data_basic import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some global variable for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_time format\n",
    "date_time_format = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "date_only_date_time_format = '%Y-%m-%d'\n",
    "\n",
    "# Data folders\n",
    "main_data_directory = '../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read all ground-truth information for each participant: IGTB, prestudy-info ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Number of user in total respond to IGTB and pre-study assessment: 212\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Input feature and label file: ../output/ml_feat/ml_input_feat.csv\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read IGTB, prestudy-info, participant id\n",
    "UserInfo = read_user_information(main_data_directory)\n",
    "\n",
    "# Read MGT\n",
    "MGT_df = read_MGT(main_data_directory)\n",
    "\n",
    "# Read work MGT\n",
    "overall_MGT, day_MGT, night_MGT = read_all_work_MGT(main_data_directory)\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Number of user in total respond to IGTB and pre-study assessment: %d' % (len(UserInfo)))\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Normalization scaler\n",
    "scaler_name = 'z_norm'\n",
    "\n",
    "# Input file path\n",
    "ml_path = os.path.join('../output', 'ml_feat', 'ml_input_feat.csv')\n",
    "\n",
    "# Output file path\n",
    "ml_output_path = os.path.join('../output', 'ml_output')\n",
    "\n",
    "if os.path.exists(ml_output_path) is False:\n",
    "    os.mkdir(ml_output_path)\n",
    "\n",
    "# Output result file path\n",
    "final_result_path = os.path.join(ml_output_path, '2_cluster_logo_rfc_all_components_' + scaler_name + '.csv')\n",
    "final_feat_importance_path = os.path.join(ml_output_path, '2_cluster_logo_rfc_feature_importance_' + scaler_name + '.csv')\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Input feature and label file: %s' % (ml_path))\n",
    "print('--------------------------------------------------------------------------')\n",
    "\n",
    "# MGT labels of interest\n",
    "lable_col_array = ['pos_af_mgt', 'neg_af_mgt', 'stress_mgt', 'anxiety_mgt']\n",
    "\n",
    "# Shift types, but if predicting all participants, we don't need to use it\n",
    "shift_array = ['day', 'night']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to choose only nurses\n",
    "prefix = 'night_nurses'\n",
    "\n",
    "if prefix == 'nurses':\n",
    "    select_users_df = UserInfo[(UserInfo['currentposition'] == 1) | (UserInfo['currentposition'] == 2)]\n",
    "elif prefix == 'non_nurses':\n",
    "    select_users_df = UserInfo[(UserInfo['currentposition'] != 1) & (UserInfo['currentposition'] != 2)]\n",
    "elif prefix == 'day_nurses':\n",
    "    select_users_df = UserInfo[(UserInfo['currentposition'] == 1) | (UserInfo['currentposition'] == 2)]\n",
    "    select_users_df = select_users_df[select_users_df['Shift'] == 'Day shift']\n",
    "elif prefix == 'night_nurses':\n",
    "    select_users_df = UserInfo[(UserInfo['currentposition'] == 1) | (UserInfo['currentposition'] == 2)]\n",
    "    select_users_df = select_users_df[select_users_df['Shift'] == 'Night shift']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Machine learning input features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read machine learning input features and labels\n",
    "def read_df(ml_path):\n",
    "    data_df = pd.read_csv(ml_path, index_col=0)\n",
    "    \n",
    "    survey_df = data_df.copy()\n",
    "    data_df = data_df.drop(['survey_time'], axis=1)\n",
    "    data_df = data_df.drop('shift', axis=1)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "# Get col name of input feature\n",
    "def get_feat_col(cols, feat_str):\n",
    "    feat_cols = []\n",
    "    for col in cols:\n",
    "        if feat_str in col:\n",
    "            feat_cols.append(col)\n",
    "    \n",
    "    return feat_cols\n",
    "\n",
    "# Select normalization method\n",
    "def select_scaler(scaler_name):\n",
    "    # Transformation\n",
    "    if scaler_name == 'z_norm':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif scaler_name == 'min_max':\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    else:\n",
    "        scaler = preprocessing.Normalizer()\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "# Read input feature and labels\n",
    "ml_df = read_df(ml_path)\n",
    "ml_select_user_df = pd.DataFrame()\n",
    "\n",
    "# ml_df\n",
    "for index, row in select_users_df.iterrows():\n",
    "    # get participant id\n",
    "    participant_id = UserInfo.loc[index, 'ParticipantID']\n",
    "    \n",
    "    \n",
    "    # aggregate data for select user\n",
    "    ml_select_user_df = ml_select_user_df.append(ml_df.loc[ml_df['participant_id'] == participant_id])\n",
    "    \n",
    "# Read col name of input feature\n",
    "feat_cols = get_feat_col(ml_select_user_df.columns.values, 'feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ramdon_forest_pred(ml_df, input_feature_col, positive_prediction_label):\n",
    "    tuned_parameters = { 'n_estimators': [50, 100, 200], \n",
    "                         'max_features': ['auto', 'sqrt', 'log2'], \n",
    "                         'max_depth': [4, 5, 6, 7, 8],\n",
    "                         'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    tuned_parameters = { 'n_estimators': [100, 200], \n",
    "                         'max_features': ['auto'], \n",
    "                         'max_depth': [7, 8],\n",
    "                         'criterion': ['gini']}\n",
    "    \n",
    "    # result df\n",
    "    final_result_df = pd.DataFrame(index=['accuracy', 'precision', 'recall', 'f1', 'MCC'])\n",
    "    feature_importance_final_df = pd.DataFrame()\n",
    "    \n",
    "    # unique subject\n",
    "    unique_subjects = ml_df['subject_idx'].unique()\n",
    "    \n",
    "    # Iterate over the grid parameters\n",
    "    for n_estimators in tuned_parameters['n_estimators']:\n",
    "        for max_features in tuned_parameters['max_features']:\n",
    "            for max_depth in tuned_parameters['max_depth']:\n",
    "                for criterion in tuned_parameters['criterion']:\n",
    "                    \n",
    "                    # a. init result for each grid parameter\n",
    "                    y_true_array, y_pred_array = [], []\n",
    "                    feature_importance_array, feature_importance_idx_array = [], []\n",
    "\n",
    "                    # b. Leave one subject out validation\n",
    "                    for unique_subject_idx, subject_idx in enumerate(unique_subjects):\n",
    "                        \n",
    "                        # 1. train df is the data with subject_idx not equal to subject_idx\n",
    "                        data_df = ml_df.copy()\n",
    "                        train_df, test_df = data_df.loc[data_df['subject_idx'] != subject_idx], data_df.loc[data_df['subject_idx'] == subject_idx]\n",
    "                        \n",
    "                        # 2. Train input\n",
    "                        x_train, y_train = train_df.drop(['subject_idx', 'label'], axis=1), train_df['label']\n",
    "    \n",
    "                        # 3. Test input\n",
    "                        x_test, y_test = test_df.drop(['subject_idx', 'label'], axis=1), test_df['label']\n",
    "                        \n",
    "                        # 4. Fit the model\n",
    "                        rfc_model = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=n_estimators, \n",
    "                                                           max_features=max_features, max_depth=max_depth, criterion=criterion)\n",
    "                        rfc_model.fit(x_train, y_train)\n",
    "                        \n",
    "                        # 5. Append feature importance of the model with one subject out\n",
    "                        feature_importance_array.append(rfc_model.feature_importances_)\n",
    "                        \n",
    "                        # 6. Append results\n",
    "                        y_true, y_pred = np.array(y_test), rfc_model.predict(x_test)\n",
    "                        for y_idx, y_true_value in enumerate(y_true):\n",
    "                            y_true_array.append(y_true_value)\n",
    "                            y_pred_array.append(y_pred[y_idx])\n",
    "                    \n",
    "                    # c. get most important feature array\n",
    "                    feature_importance_array = np.array(feature_importance_array)\n",
    "                    feature_importance_array_std = np.std(feature_importance_array, axis=0)\n",
    "                    \n",
    "                    feature_importance_array = np.sum(feature_importance_array, axis=0)\n",
    "                    feature_importance_array = feature_importance_array / len(unique_subjects)\n",
    "                    feature_importance_idx = np.argsort(feature_importance_array)[::-1]\n",
    "                    feature_importance_name_array = input_feature_col[feature_importance_idx]\n",
    "                    feature_importance_weight_array = feature_importance_array[feature_importance_idx]\n",
    "                    feature_importance_std = feature_importance_array_std[feature_importance_idx]\n",
    "                                        \n",
    "                    # d. Accuracy\n",
    "                    y_true_array, y_pred_array = np.array(y_true_array), np.array(y_pred_array)\n",
    "                    accuracy = metrics.accuracy_score(y_true_array, y_pred_array)\n",
    "                    \n",
    "                    # e. MCC\n",
    "                    MCC = matthews_corrcoef(y_true_array, y_pred_array)\n",
    "                    \n",
    "                    # f. Precision, recall, f1\n",
    "                    result = precision_recall_fscore_support(y_true_array, y_pred_array, pos_label=positive_prediction_label, average='binary')\n",
    "                    precision, recall, f1_score = result[0], result[1], result[2]\n",
    "                                    \n",
    "                    # g. save important features\n",
    "                    params_str = 'max_features_' + str(max_features) + '_max_depth_' + str(max_depth) + '_criterion_' + str(criterion) + '_n_estimators_' + str(n_estimators)\n",
    "                    \n",
    "                    final_result_df.loc['accuracy', 'cluster0'] = len(ml_df.loc[ml_df['label'] == 0])\n",
    "                    final_result_df.loc['accuracy', 'cluster1'] = len(ml_df.loc[ml_df['label'] == 1])\n",
    "                    final_result_df.loc['precision', 'cluster0'] = len(ml_df.loc[ml_df['label'] == 0])\n",
    "                    final_result_df.loc['precision', 'cluster1'] = len(ml_df.loc[ml_df['label'] == 1])\n",
    "                    final_result_df.loc['recall', 'cluster0'] = len(ml_df.loc[ml_df['label'] == 0])\n",
    "                    final_result_df.loc['recall', 'cluster1'] = len(ml_df.loc[ml_df['label'] == 1])\n",
    "                    final_result_df.loc['f1', 'cluster0'] = len(ml_df.loc[ml_df['label'] == 0])\n",
    "                    final_result_df.loc['f1', 'cluster1'] = len(ml_df.loc[ml_df['label'] == 1])\n",
    "                    final_result_df.loc['MCC', 'cluster0'] = len(ml_df.loc[ml_df['label'] == 0])\n",
    "                    final_result_df.loc['MCC', 'cluster1'] = len(ml_df.loc[ml_df['label'] == 1])\n",
    "\n",
    "                    final_result_df.loc['accuracy', params_str] = accuracy\n",
    "                    final_result_df.loc['precision', params_str] = precision\n",
    "                    final_result_df.loc['recall', params_str] = recall\n",
    "                    final_result_df.loc['f1', params_str] = f1_score\n",
    "                    final_result_df.loc['MCC', params_str] = MCC\n",
    "\n",
    "                    feature_importance_df = pd.DataFrame(index=[params_str])\n",
    "                    for i in range(15):\n",
    "                        feature_importance_df['feature_pos_name_' + str(i)] = feature_importance_name_array[i]\n",
    "                        feature_importance_df['feature_pos_weight_' + str(i)] = feature_importance_weight_array[i]\n",
    "                        feature_importance_df['feature_pos_std_' + str(i)] = feature_importance_std[i]\n",
    "                        \n",
    "                    for i in range(15):\n",
    "                        idx = len(feature_importance_name_array) - i - 1\n",
    "                        feature_importance_df['feature_neg_name_' + str(i)] = feature_importance_name_array[idx]\n",
    "                        feature_importance_df['feature_neg_weight_' + str(i)] = feature_importance_weight_array[idx]\n",
    "                        feature_importance_df['feature_neg_std_' + str(i)] = feature_importance_std[i]\n",
    "\n",
    "                    feature_importance_final_df = feature_importance_final_df.append(feature_importance_df)\n",
    "                    \n",
    "                    # Print out results for each model\n",
    "                    print(params_str)\n",
    "                    print('Validation accuracy: %.3f, precision: %.3f, recall: %.3f, f1_score: %.3f \\n' % (accuracy, precision, recall, f1_score))\n",
    "                    \n",
    "    return final_result_df, feature_importance_final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "ML model name: random_forest\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "cluster: 0, number of recordings: 202\n",
      "mgt name: pos_af_mgt, mean: 9.475, std: 3.427\n",
      "mgt name: neg_af_mgt, mean: 8.045, std: 3.457\n",
      "mgt name: stress_mgt, mean: 2.559, std: 0.949\n",
      "mgt name: anxiety_mgt, mean: 2.129, std: 0.992\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "cluster: 1, number of recordings: 184\n",
      "mgt name: pos_af_mgt, mean: 16.707, std: 4.611\n",
      "mgt name: neg_af_mgt, mean: 5.549, std: 0.999\n",
      "mgt name: stress_mgt, mean: 1.913, std: 0.796\n",
      "mgt name: anxiety_mgt, mean: 1.424, std: 0.575\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "max_features_auto_max_depth_7_criterion_gini_n_estimators_100\n",
      "Validation accuracy: 0.500, precision: 0.522, recall: 0.520, f1_score: 0.521 \n",
      "\n",
      "max_features_auto_max_depth_8_criterion_gini_n_estimators_100\n",
      "Validation accuracy: 0.495, precision: 0.518, recall: 0.510, f1_score: 0.514 \n",
      "\n",
      "max_features_auto_max_depth_7_criterion_gini_n_estimators_200\n",
      "Validation accuracy: 0.503, precision: 0.525, recall: 0.515, f1_score: 0.520 \n",
      "\n",
      "max_features_auto_max_depth_8_criterion_gini_n_estimators_200\n",
      "Validation accuracy: 0.497, precision: 0.519, recall: 0.530, f1_score: 0.525 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model types, options are svm and random_forest\n",
    "# model_types = ['svm', 'random_forest']\n",
    "model_types = ['random_forest']\n",
    "\n",
    "final_result_df = pd.DataFrame()\n",
    "\n",
    "for model_type in model_types:\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print('ML model name: %s' % (model_type))\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    \n",
    "    # For nurses only, we have shift difference\n",
    "    # for shift in shift_array:\n",
    "    \n",
    "    # Drop the recordings when there is no responses of MGT\n",
    "    ml_select_user_df = ml_select_user_df.dropna(subset=[lable_col_array])\n",
    "    ml_select_user_df = ml_select_user_df.dropna()\n",
    "    \n",
    "    # Statistics of clustering on remaining labels\n",
    "    # positive label is the cluster with higher positive affect, lower negative affect ... ...\n",
    "    mean_emotion_per_cluster_df = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        \n",
    "        data_df = ml_select_user_df.loc[ml_select_user_df['cluster'] == i][lable_col_array]\n",
    "        print('cluster: %d, number of recordings: %d' % (i, len(data_df)))\n",
    "        \n",
    "        mean_emotion_cluster_df = np.mean(data_df[lable_col_array]).to_frame().transpose()\n",
    "        mean_emotion_per_cluster_df = mean_emotion_per_cluster_df.append(mean_emotion_cluster_df)\n",
    "        \n",
    "        for affect_col in lable_col_array:\n",
    "            print('mgt name: %s, mean: %.3f, std: %.3f' % (affect_col, np.mean(data_df[affect_col]), np.std(data_df[affect_col])))\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('\\n')\n",
    "    \n",
    "    positive_prediction_label = np.argmin(np.array(mean_emotion_per_cluster_df['pos_af_mgt']))\n",
    "    \n",
    "    # 1. Seperate lables, participant id, and input features\n",
    "    subject_label = []\n",
    "    input_label, input_feature = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    for unique_id_idx, participant_id in enumerate(ml_df['participant_id'].unique()):\n",
    "        recording_data = ml_select_user_df.loc[ml_select_user_df['participant_id'] == participant_id]\n",
    "\n",
    "        # Label\n",
    "        input_label = input_label.append(recording_data['cluster'].to_frame())\n",
    "\n",
    "        # Data\n",
    "        recording_data = recording_data[feat_cols]\n",
    "        recording_data = recording_data.fillna(recording_data.mean())\n",
    "        input_feature = input_feature.append(recording_data)\n",
    "        \n",
    "        # Subject label\n",
    "        [subject_label.append(unique_id_idx) for i in range(len(recording_data))]\n",
    "    \n",
    "    # 2. Normalization\n",
    "    norm_train = np.array(np.array(input_feature[feat_cols]))\n",
    "    scaler = select_scaler(scaler_name)\n",
    "    norm_inputFeature = scaler.fit_transform(np.array(input_feature))\n",
    "    \n",
    "    # 3. ML\n",
    "    input_feature_col = input_feature.columns.values\n",
    "    \n",
    "    ml_norm_df = pd.DataFrame(data=input_feature)\n",
    "    ml_norm_df['subject_idx'] = np.array(subject_label)\n",
    "    ml_norm_df['label'] = np.array(input_label)\n",
    "    \n",
    "    results_final_df, feature_importance_final_df = ramdon_forest_pred(ml_norm_df, input_feature_col, positive_prediction_label)\n",
    "    \n",
    "    # 4. Save results to csv\n",
    "    results_final_df.to_csv(final_result_path)\n",
    "    feature_importance_final_df.to_csv(final_feat_importance_path)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Find the most import features in best performance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_final_df.drop('cluster0', axis=1)\n",
    "results_df = results_df.drop('cluster1', axis=1)\n",
    "\n",
    "feature_importance_in_best_model_df = pd.DataFrame()\n",
    "\n",
    "for index, prediction_result in results_df.iterrows():\n",
    "    \n",
    "    row_result_df = pd.DataFrame(index=['model_with_best_' + index])\n",
    "    max_component = prediction_result.max()\n",
    "    prediction_result_col = prediction_result.where(prediction_result == max_component).dropna().index.values[0]\n",
    "    \n",
    "    row_result_df['best_model_name'] = prediction_result_col\n",
    "    \n",
    "    best_results = results_df[prediction_result_col]\n",
    "    \n",
    "    for metrics, results in best_results.iteritems():\n",
    "        if 'accuracy' in metrics:\n",
    "            row_result_df['accuracy'] = results\n",
    "        elif 'precision' in metrics:\n",
    "            row_result_df['precision'] = results\n",
    "        elif 'recall' in metrics:\n",
    "            row_result_df['recall'] = results\n",
    "        elif 'f1' in metrics:\n",
    "            row_result_df['f1'] = results\n",
    "\n",
    "    row_result_df['best_score'] = max_component\n",
    "\n",
    "    model = feature_importance_final_df.loc[prediction_result_col, :]\n",
    "    \n",
    "    for model_index, model_feat in model.iteritems():\n",
    "        # row_result_df[model_index] = model_feat\n",
    "        # if 'f1' in index:\n",
    "            # if 'pos' in model_index:\n",
    "        if 'name' in model_index:\n",
    "            row_result_df[model_index] = model_feat.split('feat_')[1]\n",
    "        elif 'std' in model_index:\n",
    "            row_result_df[model_index] = model_feat\n",
    "        else:\n",
    "            row_result_df[model_index] = model_feat\n",
    "    \n",
    "    feature_importance_in_best_model_df = feature_importance_in_best_model_df.append(row_result_df)\n",
    "    feature_importance_in_best_model_df = feature_importance_in_best_model_df[row_result_df.columns.values]\n",
    "    \n",
    "feature_importance_in_best_model_path = os.path.join(ml_output_path, 'feature_importance_in_best_model.csv')\n",
    "feature_importance_in_best_model_df.to_csv(feature_importance_in_best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
